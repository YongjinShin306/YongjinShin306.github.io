---
layout: post
title: "PR-155: Exploring Randomly Wired Neural Networks for Image Recognition"
date: 2019-10-20 11:59:59
tags: [AI]
comments: true
---
### AI 논문 읽기

**Exploring Randomly Wired Neural Networks for Image Recognition**

- connection approach
- wiring is crucial
  - resnet
  - densenet
- NAS(Neural Architecture Search)
  - find the connection
- `However` 
  - 입력을 어디서 가져올건지 뽑아주는데(h_i, h_i-1)
  - concat, add는 사람이 constraint를 걸었다
  - constraint를 빼면 어떻게 할 것이냐?

**Main Topic**
- NAS 같은 것을 여러개 만들어보자
- Generator를 어떻게 Design할 것인지 생각해보자
  - 한 단계 높은 수준의 Abstract

**Related Work**
- CNN & RNN use chain-like wiring patterns.
  - depthwise conv 같은 operation 에 대한 고민 x
  - wiring을 어떻게 잘 할 것인지
- Neural Architecture Search(NAS)
  - Operation 종류는 변하지 않고 있다
- Randomly Wired Machine
- Neuro Science 와 관련 있다
- Random Graphs in Graph Theory
  - Random Graph 라도 완벽한 Random 은 아니고, Prior가 들어가 있다
  - Prior Knowledge가 Encoding되어 들어가 있다

**Network Generator**
- $\theta=depth,width,filter,activation$
- Stochastic Network Generator
  - hand-desinged and encodes a prior from human knowledge
- Network Generator를 여러개 들어서 어떻게 작동하는지 이해 필요
- Generating General Graphs
  - Starting by generating a general graph.
  - without restict
- Edge Operation (Mapping 을 어떻게 할 것인지)
  - Aggregation
    - ![](/images/2019-10-24-14-18-11.png)
  - ReLU-Conv-BN triplet
    - weight(positive)
    - 값이 커지는 것을 막기 위해, BN을 제일 마지막에
  - Only Depthwise Separable Conv
    - Wiring에 Focus 맞추기 위해
- Nice Properties
  - Addictive Aggreagtion maintains the same number of output channels as inptut channel
  - 어떤 node 와도 연결 가능
  - FLOPs and parameter count unchanged
- Input and Output Node
  - 우리는 현재 single Input and Output
    - Extra Node 추가
    - Output -> Average
- Stages
  - ![](/images/2019-10-24-14-29-29.png)