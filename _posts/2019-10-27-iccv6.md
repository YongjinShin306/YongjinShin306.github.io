---
layout: post
title: "ICCV Workshop Interpretable Machine Learning for Computer Vision Summary - (6)"
date: 2019-10-27
tags: [AI]
comments: true
---
### Explaining Deep Learning for Identifying Strctures

- Examples: small-world graphs
- From a graph to a neural network
- Process
  - general graph
  - convert to DAG
  - add extra input & output node
  - concat
- Wiring pattern matters
- Network wiring is orthogonal to the chosen operations.
- NAS vs Amobeanet
- Conclusion
  - randomly wird network can work ery well
  - waring pattern matters

### Progressive Differential Architecture Search
  - Search space approicmation
  - Neural Architecture Search (NAS)
  - DARTS
  - Motivation
    - Depth gap : inconsistency between search and evaluation
    - Instability: searched network (shallow) and not necessarily good in evaluation (deep)
  - `Solution`
  - Search Space Approximation
    - `progressively increasing depth` meanwhile reducing search space
  - Regularization
    - skipcoonect dominates
      - operation-level dropout 
    - varing numbers of skip-connect
      - fixing the number of skip-coonect ...
  - Searched Architecture
  - Conclusion
    - PDARTS
      - Briding the depth gap
      - stabilize search space


### Multinomial Distribution (MdeNAS)
- What is important in NAS?
- Search Space
  - B1,B2,B3,B4
- pdf
- sampling
- training
- unformation recording
- pdf update
- transferable to imagenet
  - darts <=
- Search on ImageNet
- GPU latency

### Searching for MobileNetV3
-  Mobile -> Resource Constrained
-  Small and Large
-  Search Algorithms
   -  MNasNet ( consider latency)
   -  `NetAdapt`
-  New Non-Linearity: Hard-Swish
   -  **Swish**
-  New Classification HEad
   - GAP + 1x1 Conv
- Mobile + DeepLab = Mobile DeepLab

### Data Free Quantization
- Quantization is model specific
- imbalance in weight ranges per output
- Cross-layer Equalization (CLE)
- Bias absorption
- Quantization error can biased
- Bias Correction
  - Solution: Subtract biased error (used bn)
- Data-Free Quantization (DFQ)
  - Cross-layer equalization
  - bias correctoin
- Conclusion
  - introducded data-free quantization method

### A Camera
- SCAMP5: Pixwl Processor Array


### Convolution Storage and Pooling
- Network Training
- Leverage  PPA in computing convolutional alyers

### knowledge Distillation via Route Constrained Optimization
- T -> S
- RCD
- `with stride = x(EEI-x)`
- **by PCA direction**

Future Work

### Distillation based training ...
- Anytime Prediction
- Multi-Exit Architecture
- **Distillation-based training**
- CLassification loss
- Distillation loss
- Disillation-based training
  - L_cls + L_dist
- Adaptive temperature annealing
- Architecture: multi-scale densenet
  - ours: 3+
  - baseline: only classification loss
- result
  - exit 1 ~ exit 5 에 대해 성능이 전부다 높음
- Summary: Distillation-based training

### similarity-preserving knoledge distillation
- Supervised Learning
- Knowledge distllation
- Neural network cpression
- learning with private or privileged data
- adversarial defense
- ...and so on
- Semantically similar inputs tend to elicit similar activation patterns ...
- Teacher - Student 구조
- Student 가 똑똑한게 낫구만..

### Task Routing
- Architecture Specific Multitask learning approaches
- domain expertise required
- limited task count per model
- gifh resource consumption and 
- multi task learning
- task routing networks
- task 수에 따라, 성능이 일정하게 됨...

### ~
- MTL -> feature sharing
- Stochastic Filter Groups
- SFG optimizttion
- gumbel softmax
- Multi Task Specific Leraning
- Activation Visualization 

### Transferability and Hardness of ...
- 잘 transfer?
- target task
- task2vec
- Conditional Entropy?
- 지린다...
- poster 12

### Moment Matching for Multi-Soucre Domain Adaptation
- Sketch Paining Clipart
- DA Dataset Want
- 3000 hours 
- Moment MAtching for Multi SOucr DOmain Adaptation

Validtaion
- Digit Five
- OFfice-Caltech

### Unsupervised Domain Adaptation ...
- Source -> Target
- for UDA
- DANN
- Encoder
- Conditional ALignment
- Co-DA

### Larger Norm More Transferable: 
- DOmain Adaptation
- tsne
- motivation
  - two hypothesis
    - misalgined-feature-norm hypothesis
- Task specific 
- L2_preserved Dropout
- HAFN
  - Hard Adaptive Feature NOrm
- Discussion of HAFN
- SAFN
  - Stepwise Adaptive Fetaure Norm
- Partial Setting
- Summary


### ~
- depth
- task-transfer network
- unsupervised multi-task adaptation


### Episodic Training for Domain Generalizaiton
~~


### 
1. pixel-lvel
2. pseudo label training
3. feature trainign

- Patch Mode Discovery
- ...
- ...

Patch ALigbnebt
- w/o patch level alighment

Output Space Adapatation 등에대해서도
patch 다 성능 오름
- patch level adaptation framework
- discover patch modes and learn patch representation
- perform patch-level alignment

### SSL
- Identical Distribution -> Identical Empirical Distribution?
- Empricail Distribution Mismatch -> biased prediction
- Pseudo Label
- discriminator(label/unlabel)
- Too few labeled augmentation
- cross-set sample augmentation
  - more samples
- ADA-NEt
  - Both Components help
  - ...
  - `Adversarial`

### $S^4L$ 
- Sample efficient image classification
- self-supervision
- exemplar rotation
- self supervised
- semi supervised
- Rotation, Exemplar
- MOAM
  - S4LRotation +VAT+EntMin
  - Pseudo Labeling
  - Fine Tuning
- S4L Poster 20
- 