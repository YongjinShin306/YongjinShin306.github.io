---
layout: post
title: "ICCV Workshop Interpretable Machine Learning for Computer Vision Summary - (2)"
date: 2019-10-27
tags: [AI]
comments: true
---
### Deep Neural Network Explanation

1. Analysis
   - How it works and how it learns
2. Win an aurgment
   - convincing her
3. Communicating a skill
   - Explain to a human or machine how to solve a certain class of problems, in general

**Analysis deep neural networks**
What does a net do?
- What Concepts can it recognise?
- spurious correaltions

How does it do it?
- template matching?
- compositionaly


How does it learn it?
- generalization
- optimisation

Deep networks as encoder

1. **Generating iconic examples**
2. **Attributions**

### Generating iconic examples

How much information about x does y contain?

**Pre-image**
- Reconstructions form an equivalence class of images, called a pre-image
- All pre-images hat are indistinguishable for the network

**Finding pre-images via optimization**

**Natural Pre-images**

**Pseudo-natural pre-images**
- regularised energy
- **constrained optimisation**
- posterior probability

Generator nets as image parameterisations
- Consider a **generator network** with a fixed input $z_0$
- The network parameters w can be thought as **image parameters**

Fit a network to a single example

Start **randomly-initialised** network

**Deep image prior**
- For most generator networks fitting naturally-looking images is easier/faster than fitting others.
- for inpainting we only reconstruct the visible pixel

**Inverting**
- Activation Maximization...


### Attributions
- Deconvolution
- Bakprop Method: Gradient
- Guided Backpropagation

Backprop: deconv, grad, guided backprop

**Comparisons**
- Lack of channel specificity
  - maximally activated neuron
  - random neuron
  - minimally neuron
    - same result
- Backprop: CAM and Grad-CAM
- Relevance and excitation backprop

**The meaninf of attribution maps**
- Grad Method = Sensitivity Analysis
- local linear approximation

**Purturbation Analysis**
- Change the input and observe the effect on the output
- Occlusion
- RISE

**Extemal Perturbations**
- Preserve 10% => response preserved
- Meaningful perturbations
- Smooth Masks
- Comparison with prior work on "meaningful ..."

**Result**
- Foreground evidence is usually sufficient
- Diagnosing networks

**Assessing attribution**
- poing game & weak localisation
- **Goal**: measure the spatial correaltion between attribution maps and object occurrences

If the correaltion is strong:
- the gianosed model "understand" the object **and**
- the attribution method can tell

Assessing attribution: neuron sensitivity
- Attribution should also produce a different output if the model weights are ... 
- Shift Invariance

Channel Attribution

### Summary
**Generating conic examples**
- inversion vs activation maximization
- the importance of the prior / regularization
- aesthetic vs diagnostic

**Attribution**
- (Modified) gradient backpropagation
- Excitation and relevance backpropagation
- Meaningful perturbation anaylysis
- Understanding via approximating models