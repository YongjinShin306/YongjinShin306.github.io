---
layout: post
title: "ICCV Workshop Interpretable Machine Learning for Computer Vision Summary - (5)"
date: 2019-10-27
tags: [AI]
comments: true
---
### Explaining Deep Learning for Identifying Strctures

What is a possible explanation of a prediction? for images:
- Densenet121, Keras+ pretrained model, 2019
  - case of images: compute a score for every pixel
- case of images: compute a score for every pixel
  - patch-wise classifciation: label = 1 if patch contains breast cancer
  - pixel wise explanation
- general case: score for every dim of an inpute sample
  -  $x =(x_1, ..., x_d, x_D)$
-  What is LRP as explanation?
   -  image
   -  gradient
   -  $LRP-\alpha-\beta$
-  Theorical Interpretation
-  Trivial Rules

**Deep Taylor Decomposition**
- LRP's idea: To robustly explain a model, leverage the neural network strcture of the decision function.

**Relevance distribution for one neuron: example $e-rules$**
- $e-$dakmping factor, numerical stabilization
- recommended for fully connected layers and good for LSTMs
- NOT recommended for conv layer
- $\beta-rule$
  - $\beta-$ controls ratio of negative to positive ...

**Gradient x Input**
- Observation: Complex analysis reduce to gradient x input
- very noisy explanation

**Two reasons why explanations are noisy**
- Not local enough. Too much context
- The Shattered gradients problem
- Examples with hybrid rule: $\beta=0$ for conv layer, $\epsilon$

**LRP Applied to Variety of Models**
- Conv Net
- Bow Model
- LSTM
- One-class SVM

**LRP Applied to Variety of Tasks**

**The value of explanation**
- application case: identify action strategies in reinforcement learning predictors
- Deep RL

**LRP: DNN and Atari Breakout**
- LRP can help to discover stratigies: building a tunnel -evolution during training
- LRP can help to find parameters for fast learning of know strategies. Here: impact of $M$ = replay memory size
- Interpretability methods (LRP) can uncover complex relationship.

**Identify Biases in Train + test Data (where labels do not help you at all)**
- `Anaylzing Claaisfier: Fisher Vectors and Deep Neural Networks (CVPR 2016)`

**SpRAy: semi-automatic discovery of correlations**
- Lapuschkin et al. Nature Communications 2019:
- Principles
  - Compute heatmaps, pool them into a uniform low resolution
  - compute
- DNN and Pascal VOC Aeroplane class
- t-sne shows one cluster where aeroplanes have strong evidence on edges due to data preparation artefact combined with frquency of blue sky.
- Did not wanted to use center crops: avoid cutting off object parts. So edges were padded with border pixels. This is used in one part.

Quantifying heatmaps on cell level
- Three datasets: Annotate nuclei densely.
- Train patch classifier
- vs Grad-CAM

Sampling bias
- necrosis sampling
  - label and test set cannot 
- Class Correlation Bias
  - training image
  - test image
  - heatmap w/ bias
  - heapmap w/o bias
- biases are indentifiable
- test set labels no helpful

**Image Scaling**
- 80% 하니까 더 좋을 수도 있구나..?

Identify Fail Cases without labelling efforts
- More Importanly:
  - Decide what unlabeled data to add into next iteration of trian and test set - precursor ...



Book: Explainable AI: INterpreting, Explaining and Visualizing Deep learning

thesis: methods for interpreting and understanding deep nueral networks
code: https://github/com/albermax
site: www.explainable-ai.org