---
layout: post
title: "ICCV Workshop Interpretable Machine Learning for Computer Vision Summary - (4)"
date: 2019-10-27
tags: [AI]
comments: true
---
### Deep Compositional Networks 

**Abstract**
- this talk describes a family of compositional networks which are interpretable ...

**Background**
- Deep Nets are hard to interpret and have unusual failure modes inparticular they are sensitive to occlusion and context

Part 1: Visual Concepts: Internal Representations
- We study internal representations within Deep Nets.
- study vehicles at fixed scale 

**Background**
- deep nets contain internal representations represented by neural featues. the findings included:
  - if deep nets are trained to

**Visual Concepts**
- Clustering
  - k-means (k=200)
- Perceptually Tight
  - Finding 1: The visual concepts were perceptually tight. Image patches corresponding to the same visual concept are very similar.
  - We sgiw the closest 6 image patches (left), a random sample of 6 patches from the top 500  ...
- Coverage of the Object

**To Explore: We Annotate Semantic Parts.**
- We annotated the vehicles in PASCAL 3D + Toc create the Vehicle Semnatic ...

**VCs as Key-Point, Semantic Part Detectors**
- VCs were fairly good for detecting key-points and semantic parts of the vehicles. but much worse than supervised models.
- Key-Ponts. 13 K-Ps for Bike
- Semantic Parts. Yellow bars show the bset APs for each VC.

**VC detect subparts of Semantic Parts**
- VCs can act as unsupervised detectors for key-points and semantic-parts. Their Average Precisiosns (APs) are weaker than supervised methods.
- We observe that most VCs respond to several different semantic parts (typically 1-4). The VCs correspond to subparts of semantic parts (which are shared).

**Combine VCs to detect Semnatic Parts**
- We design a compositional model for detecting semantic parts. Each model consists of a set of VCs which fire in different spatial positions. (Illustrated for object-car-instead of semantic part)
- Compositional Voting: each VC votes for the ...

**Semantic Part Detection with `Occlusion`**
- Vehicle Occlusion Dataset
- deep nets have difficulty with occlusion. But compositional voting is likely to be most robust. the occluded VC will not respond, but the **un-occluded VCs will still vote**.
- **Compositional voting** also includes **context**, image information outside the semantic part, because this is also robust

Detecting Semantic Parts with Occlusion
- In the occlusion dataset semantic parts
  - fully occluded (red)
  - partillay occluded (blue)
  - un-occluded (yellow)
- Compositional voting uses VCs on and off the semantic parts. If a VC is detected (green) then it votes for the semantic part. If a VC is occluded (red) then it gives no vote.
- Note: a semantic part can be detected even if it is fully occluded.

Compositional Voting: Detect Semantic Parts
- The compositional voting method (VT) outperforms alternatives like Deep Nets if there is **significant occlusion**
- Main Idea: explicit representation of subparts (by VC) enables the algorithm to switch them on and off automaticlly.

**Visual Concepts: Summary**
- The Deep Nets encode representations of the parts. These are stroed 
- Making this representation explicit - e.g., by compositional voting - enables us to detect semantic parts despite heavy occlusion. The algorithm can automatically switch off subparts (VCs) if they are not detected in the correct locations.
- It is harder for deep nets 

**Part 2: Compositional Nets for OBject Classification**
- Can Deep Nets be modified to produce better internal representations corresponding to object parts?
- there has been some work in this direction.
- `Learning deep parsimonious representation NIPS 2016`


**Deep Networks and Occlusion**
- Deep Nets performance degrades on occluded objects.
- Experiments: Train on un-occluded data and test on Occluded.
- (Why? Because there are an exponential number of ways to occlude objects. See Neural ARchitetcure talk 28/Oct)

**Different Type Occluders**
- Compositional Nets
  - compositional voting models only work for fixed viewpoint.
  - Object appearance depends on the viewpoint. Different VCs will be activated for different viewpoints and in different spatial locations.
  - This require us to use mixture models for objects.

**Two Models: CompNet-Dict & CompNet-Full Hard-VCs and Soft-VCs**
- We describe two types of models for each mixture component.
- The models are generative: (i) ...

**CompNet-Dict: Generative Model for Hard-VCs**
**Occlusion and Robustness**
- to enable the generative model robust - i.e. able to deal with occlusion by allowing a probability 

**The mixture model**
- An object is represented by a mixture of distributions.

**CompNet-Full. Generative Model for Hard-VC encoding**
- Generative models are learnt for all objects.
- The only supervision is object identity. The learning algorithm involves ~

**Von Mises-Fisher Distribution**
- Bernoulli Distribution
- Feature vector  normalized to lie on unit sphere.

**Compare CompNet-Dict, CompNet-Full, and Deep Net (VGG)**
- Both CompNet models do than Deep Nets as the Occlusion increases.
- CompNet-Full (soft-VCs) slightly outperforms CompNee-Dict (hard VCs), but both outperforms VGG

### Summary
- CompNet
- Part 1. We started the internal representations within depe nets, by using clustering to detect Visual Concepts (VCs).
- This validated that deep nets had internal representations of object parts. We showed that the VCs could be used to detect key-points and semantic parts.
- We showed that compositional models -VC plus spatial relations - could detect semantic parts better than deep nets if there was significant occlusion.
- Part 2. We developed CompoNet architecture that could classify vehicles with significant occlusion. These models were interpretable. They could detect/localize occluders, localize subpart (VCs).
- See poster in Neural Architecture Workshop (28/Oct).